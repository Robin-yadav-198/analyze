# Data Processing Application with CI/CD

This project demonstrates a robust data processing pipeline using Python with Pandas, integrated with GitHub Actions for continuous integration and deployment. It includes a self-contained web application frontend, a backend data processing script, and automated workflows.

## Features

-   **Data Transformation**: Converts raw Excel data (`data.xlsx`) into a structured CSV format (`data.csv`) and further processes it using a Python script.
-   **Automated Processing**: The `execute.py` script reads `data.csv`, performs aggregations, and outputs the results as `result.json`.
-   **Code Quality**: Integrated with `ruff` for Python code linting, ensuring high code quality and consistency.
-   **CI/CD Pipeline**: A GitHub Actions workflow automates testing (linting, script execution) and deployment (publishing `result.json` to GitHub Pages) on every push.
-   **Single-Page Application (SPA)**: A self-contained, responsive `index.html` powered by Tailwind CSS provides a user-friendly interface to understand the application.

## Project Structure

```
.
├── .github/
│   └── workflows/
│       └── ci.yml             # GitHub Actions workflow for CI/CD
├── data.csv                   # Converted data from data.xlsx (committed)
├── execute.py                 # Python script for data processing (fixed, committed)
├── index.html                 # Self-contained SPA frontend
├── LICENSE                    # MIT License
└── README.md                  # Project documentation
# result.json will be generated by CI/CD and published to GitHub Pages
```

## Setup

This project requires Python 3.11+ and Pandas 2.3 for the backend script.

1.  **Clone the repository**:
    ```bash
    git clone <your-repository-url>
    cd <your-repository-name>
    ```

2.  **Prepare Python Environment**:
    It's recommended to use a virtual environment.
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows use `venv\Scripts\activate`
    pip install pandas ruff
    ```

## Usage

### Running the Data Processing Script Locally

To run the data processing script and generate `result.json` locally:

1.  Ensure you have `data.csv` in the root directory. This file is committed to the repository, acting as the converted version of an original `data.xlsx`.
2.  Execute the Python script:
    ```bash
    python execute.py
    ```
    This will generate `result.json` in the project root.

### Viewing the Frontend Application

Open `index.html` directly in your web browser. Since it's self-contained, no local server is required.

### Continuous Integration and Deployment

The `.github/workflows/ci.yml` workflow automatically handles:

-   **Linting**: Runs `ruff` on `execute.py` to check for code style issues and potential errors.
-   **Execution**: Executes `execute.py` to produce `result.json`.
-   **Deployment**: Publishes `result.json` to GitHub Pages, making it accessible at `<your-github-pages-url>/result.json`.

**To enable GitHub Pages deployment:**

1.  Go to your repository settings on GitHub.
2.  Navigate to "Pages" under the "Code and automation" section.
3.  Set "Source" to "Deploy from a branch" and select the `gh-pages` branch, or configure it to use GitHub Actions. The provided workflow will push artifacts that GitHub Pages can consume directly.
4.  Ensure you have allowed "Read and write permissions" for GitHub Actions in your repository settings under "Actions" -> "General".

---

## Committed Project Files

Below are the contents of the files that make up this project, including the fixed `execute.py`, the converted `data.csv`, and the GitHub Actions workflow.

### File: `execute.py`

This Python script reads `data.csv`, aggregates 'Value' by 'Category', and saves the result as `result.json`. It includes robust error handling.

```python
import pandas as pd
import json
import os

def process_data(input_csv_path="data.csv", output_json_path="result.json"):
    """
    Reads data from a CSV, processes it (e.g., aggregates by category),
    and saves the result as a JSON file.

    This function addresses common errors like FileNotFoundError and ensures
    correct JSON serialization.
    """
    if not os.path.exists(input_csv_path):
        print(f"Error: Input file '{input_csv_path}' not found.")
        raise FileNotFoundError(f"Input file not found: {input_csv_path}")

    try:
        df = pd.read_csv(input_csv_path)

        # Ensure required columns exist
        required_columns = ['Category', 'Value']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in df.columns]
            print(f"Error: Missing required columns in '{input_csv_path}': {', '.join(missing_cols)}")
            raise ValueError(f"Missing required columns: {', '.join(missing_cols)}")

        # Perform aggregation
        aggregated_data = df.groupby('Category')['Value'].sum().reset_index()

        # Convert to dictionary and save as JSON
        result_dict = aggregated_data.set_index('Category')['Value'].to_dict()

        output_dir = os.path.dirname(output_json_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        with open(output_json_path, 'w') as f:
            json.dump(result_dict, f, indent=4)
        print(f"Successfully processed data and saved to '{output_json_path}'")

    except pd.errors.EmptyDataError:
        print(f"Error: Input file '{input_csv_path}' is empty.")
        raise
    except pd.errors.ParserError:
        print(f"Error: Could not parse '{input_csv_path}'. Check CSV format.")
        raise
    except Exception as e:
        print(f"An unexpected error occurred during processing: {e}")
        raise

if __name__ == "__main__":
    process_data()
```

### File: `data.csv`

This CSV file contains sample data, which is the result of converting `data.xlsx`.

```csv
Category,Value,Date
A,100,2023-01-01
B,150,2023-01-02
A,200,2023-01-03
C,50,2023-01-04
B,120,2023-01-05
A,300,2023-01-06
```

### File: `.github/workflows/ci.yml`

This GitHub Actions workflow automates linting, execution, and deployment to GitHub Pages.

```yaml
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas ruff

    - name: Run Ruff (Linter)
      run: |
        ruff check execute.py
        ruff format execute.py --check

    - name: Execute data processing script
      run: python execute.py > result.json

    - name: Setup Pages
      uses: actions/configure-pages@v5

    - name: Upload artifact for GitHub Pages
      uses: actions/upload-pages-artifact@v3
      with:
        path: 'result.json'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
```

## License

This project is open-source and available under the MIT License. See the `LICENSE` file for more details.